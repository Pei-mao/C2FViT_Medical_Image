{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "910a719a-4304-4498-b156-dd013772249d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model weight ../Model/C2FViT_affine_COM_template_matching_tigerdata_RAS/C2FViT_affine_COM_template_matching_tigerdata_RAS_stagelvl3_249000.pth ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1367/1937543338.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model has been converted to mprage_rigid_v001_train_custom.onnx\n"
     ]
    }
   ],
   "source": [
    "#model轉onnx(C2FViT)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from C2FViT_model import C2F_ViT_stage, AffineCOMTransform, CustomAffineCOMTransform, Center_of_mass_initial_pairwise, CustomCenter_of_mass_initial_pairwise\n",
    "from Functions import min_max_norm, pad_to_shape\n",
    "\n",
    "class FullModel(nn.Module):\n",
    "    def __init__(self, model, affine_transform, init_center):\n",
    "        super(FullModel, self).__init__()\n",
    "        self.model = model\n",
    "        self.affine_transform = affine_transform\n",
    "        self.init_center = init_center\n",
    "\n",
    "    def forward(self, moving_img, fixed_img):\n",
    "        # Center of mass initialization\n",
    "        moving_img, init_flow = self.init_center(moving_img, fixed_img)\n",
    "        \n",
    "        # Downsample the images\n",
    "        X_down = F.interpolate(moving_img, scale_factor=0.5, mode=\"trilinear\", align_corners=True)\n",
    "        Y_down = F.interpolate(fixed_img, scale_factor=0.5, mode=\"trilinear\", align_corners=True)\n",
    "        \n",
    "        # Run the core model\n",
    "        warpped_x_list, y_list, affine_para_list = self.model(X_down, Y_down)\n",
    "        #rigid\n",
    "        #affine_para_list[-1][0, 11] = 0\n",
    "        #affine_para_list[-1][0, 10] = 0\n",
    "        #affine_para_list[-1][0, 9] = 0\n",
    "        #affine_para_list[-1][0, 8] = 0\n",
    "        #affine_para_list[-1][0, 7] = 0\n",
    "        #affine_para_list[-1][0, 6] = 0\n",
    "        # Apply the affine transformation\n",
    "        X_Y, affine_matrix = self.affine_transform(moving_img, affine_para_list[-1])\n",
    "        \n",
    "        return X_Y, affine_matrix, init_flow\n",
    "\n",
    "# 設定裝置\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 定義模型\n",
    "model = C2F_ViT_stage(img_size=128, patch_size=[3, 7, 15], stride=[2, 4, 8], num_classes=12,\n",
    "                      embed_dims=[256, 256, 256], num_heads=[2, 2, 2], mlp_ratios=[2, 2, 2], qkv_bias=False,\n",
    "                      qk_scale=None, drop_rate=0., attn_drop_rate=0., norm_layer=nn.Identity,\n",
    "                      depths=[4, 4, 4], sr_ratios=[1, 1, 1], num_stages=3, linear=False).to(device)\n",
    "\n",
    "# 加載預訓練模型權重\n",
    "model_path = '../Model/C2FViT_affine_COM_template_matching_tigerdata_RAS/C2FViT_affine_COM_template_matching_tigerdata_RAS_stagelvl3_249000.pth'\n",
    "print(f\"Loading model weight {model_path} ...\")\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "# 定義轉換器\n",
    "affine_transform = AffineCOMTransform().to(device)\n",
    "#affine_transform = CustomAffineCOMTransform().to(device)\n",
    "init_center = Center_of_mass_initial_pairwise()\n",
    "#init_center = CustomCenter_of_mass_initial_pairwise()\n",
    "\n",
    "# 將核心模型和轉換器打包成完整模型\n",
    "full_model = FullModel(model, affine_transform, init_center).to(device)\n",
    "\n",
    "# 加載固定影像\n",
    "fixed_path = '../Data/MNI152_T1_1mm_brain_pad_RSP.nii.gz'\n",
    "fixed_img_nii = nib.load(fixed_path)\n",
    "fixed_img = fixed_img_nii.get_fdata()\n",
    "\n",
    "# 確保影像尺寸是 256x256x256\n",
    "target_shape = (256, 256, 256)\n",
    "if fixed_img.shape != target_shape:\n",
    "    fixed_img = pad_to_shape(fixed_img, target_shape)\n",
    "\n",
    "fixed_img = min_max_norm(fixed_img)\n",
    "fixed_img = torch.from_numpy(fixed_img).float().to(device).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "# Dummy moving image for ONNX conversion\n",
    "dummy_moving_img = torch.randn(1, 1, 256, 256, 256).to(device)\n",
    "\n",
    "# 將完整模型轉換成 ONNX 格式\n",
    "onnx_path = \"mprage_affine_v001_train.onnx\"\n",
    "#onnx_path = \"mprage_affine_v001_train_custom.onnx\"\n",
    "torch.onnx.export(full_model, \n",
    "                  (dummy_moving_img, fixed_img), \n",
    "                  onnx_path, \n",
    "                  export_params=True, \n",
    "                  opset_version=20, \n",
    "                  do_constant_folding=True, \n",
    "                  input_names=['moving_img', 'fixed_img'], \n",
    "                  output_names=['moved', 'affine_matrix', 'init_flow'])\n",
    "\n",
    "print(f\"Full model has been converted to {onnx_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6d1e1c50-3c75-4b2c-ad7b-058055187e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 256, 256, 256)\n",
      "(1, 3, 4)\n",
      "[[ 0.8344236   0.24871936  0.01360709  0.01237769]\n",
      " [-0.18627378  0.9458701  -0.00467041 -0.01208516]\n",
      " [-0.014078    0.03331293  0.71046937  0.00340794]]\n",
      "(1, 256, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "#讀取資料並預測\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from Functions import min_max_norm, pad_to_shape, reorient_image\n",
    "\n",
    "    \n",
    "moving_nii = nib.load('/NFS/PeiMao/dataset/ABIDE_NoAffine/ABIDE_0050424_tbet.nii.gz')\n",
    "fixed_nii = nib.load('/NFS/PeiMao/GitHub/C2FViT_Medical_Image/Data/MNI152_T1_1mm_brain_pad_RSP_RAS.nii.gz')\n",
    "fixed_affine = fixed_nii.affine\n",
    "fixed_header = fixed_nii.header\n",
    "moving_nii = reorient_image(moving_nii, ('R', 'A', 'S'))\n",
    "moving_data = moving_nii.get_fdata().astype(np.float32)\n",
    "fixed_data = fixed_nii.get_fdata().astype(np.float32)\n",
    "moving_data = pad_to_shape(moving_data, (256, 256, 256))\n",
    "\n",
    "fixed_data = np.clip(fixed_data, a_min=2500, a_max=np.max(fixed_data))\n",
    "\n",
    "# 在第0轴和第1軸位置添加新维度（增加一个 batch size 维度）\n",
    "moving = np.expand_dims(moving_data, axis=0)\n",
    "moving = np.expand_dims(moving, axis=1)\n",
    "fixed = np.expand_dims(fixed_data, axis=0)\n",
    "fixed = np.expand_dims(fixed, axis=1)\n",
    "\n",
    "moving = min_max_norm(moving)\n",
    "fixed = min_max_norm(fixed)\n",
    "\n",
    "# 创建 ONNX Runtime 会话\n",
    "session = ort.InferenceSession(\"mprage_affine_v001_train.onnx\")\n",
    "\n",
    "# 获取输入的名称\n",
    "input_names = [input.name for input in session.get_inputs()]\n",
    "output_names = [output.name for output in session.get_outputs()]\n",
    "\n",
    "# 创建输入字典\n",
    "inputs = {input_names[0]: moving, input_names[1]: fixed}\n",
    "\n",
    "# 运行模型推理\n",
    "outputs = session.run(None, inputs)\n",
    "\n",
    "# 打印输出结果\n",
    "print(outputs[0].shape)\n",
    "print(outputs[1].shape)\n",
    "\n",
    "# 使用 squeeze 移除长度为 1 的维度\n",
    "moved = np.squeeze(outputs[0])\n",
    "affine_matrix = np.squeeze(outputs[1])\n",
    "init_flow = outputs[2]\n",
    "print(affine_matrix)\n",
    "print(init_flow.shape)\n",
    "\n",
    "# 创建一个 NIfTI 图像对象\n",
    "moved_nii = nib.Nifti1Image(moved, fixed_affine)\n",
    "\n",
    "# 保存 NIfTI 图像为 .nii.gz 文件\n",
    "#nib.save(moved_nii, 'onnx_output_image_rigid.nii.gz')\n",
    "nib.save(moved_nii, 'onnx_output_image_aff.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9fd8400e-1533-46f4-a5fd-ab3992065473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model has been converted to mprage_affine_transform_v001_train.onnx\n"
     ]
    }
   ],
   "source": [
    "# #model轉onnx(C2FViT)(affine transform)\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import nibabel as nib\n",
    "# import numpy as np\n",
    "# from C2FViT_model import AffineCOMTransform, Center_of_mass_initial_pairwise\n",
    "# from Functions import min_max_norm, pad_to_shape\n",
    "\n",
    "# class FullModel(nn.Module):\n",
    "#     def __init__(self, init_center):\n",
    "#         super(FullModel, self).__init__()\n",
    "#         self.init_center = init_center\n",
    "\n",
    "#     def forward(self, moving_img, fixed_img, moving_seg, affine_matrix):\n",
    "#         # Center of mass initialization\n",
    "#         moving_img, init_flow = self.init_center(moving_img, fixed_img)\n",
    "        \n",
    "#         # Downsample the images (not needed for segmentation transformation)\n",
    "#         # X_down = F.interpolate(moving_img, scale_factor=0.5, mode=\"trilinear\", align_corners=True)\n",
    "#         # Y_down = F.interpolate(fixed_img, scale_factor=0.5, mode=\"trilinear\", align_corners=True)\n",
    "        \n",
    "#         # Apply the grid sample for center of mass alignment\n",
    "#         moving_seg = F.grid_sample(moving_seg, init_flow, mode='nearest', align_corners=True)\n",
    "        \n",
    "#         # Generate the affine grid and apply the affine transformation\n",
    "#         F_X_Y = F.affine_grid(affine_matrix, moving_seg.shape, align_corners=True)\n",
    "#         moving_seg = F.grid_sample(moving_seg, F_X_Y, mode='nearest', align_corners=True)  # Use 'nearest' for segmentation\n",
    "        \n",
    "#         return moving_seg\n",
    "\n",
    "# # 設定裝置\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # 定義轉換器\n",
    "# init_center = Center_of_mass_initial_pairwise()\n",
    "\n",
    "# # 將轉換器打包成完整模型\n",
    "# full_model = FullModel(init_center).to(device)\n",
    "\n",
    "# # 加載固定影像\n",
    "# fixed_path = '../Data/MNI152_T1_1mm_brain_pad_RSP_RAS.nii.gz'\n",
    "# fixed_img_nii = nib.load(fixed_path)\n",
    "# fixed_img = fixed_img_nii.get_fdata()\n",
    "\n",
    "# # 確保影像尺寸是 256x256x256\n",
    "# target_shape = (256, 256, 256)\n",
    "# if fixed_img.shape != target_shape:\n",
    "#     fixed_img = pad_to_shape(fixed_img, target_shape)\n",
    "\n",
    "# fixed_img = min_max_norm(fixed_img)\n",
    "# fixed_img = torch.from_numpy(fixed_img).float().to(device).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "# # Dummy moving image, segmentation, and affine matrix for ONNX conversion\n",
    "# dummy_moving_img = torch.randn(1, 1, 256, 256, 256).to(device)\n",
    "# dummy_moving_seg = torch.randint(0, 2, (1, 1, 256, 256, 256)).float().to(device)  # Assuming binary segmentation\n",
    "# dummy_affine_matrix = torch.tensor([[[0.8344236, 0.24871936, 0.01360709, 0.01237769],\n",
    "#                                      [-0.18627378, 0.9458701, -0.00467041, -0.01208516],\n",
    "#                                      [-0.014078, 0.03331293, 0.71046937, 0.00340794]]]).to(device)  # Shape (1, 3, 4)\n",
    "\n",
    "# # 將完整模型轉換成 ONNX 格式\n",
    "# onnx_path = \"mprage_affine_transform_v001_train.onnx\"\n",
    "# torch.onnx.export(full_model, \n",
    "#                   (dummy_moving_img, fixed_img, dummy_moving_seg, dummy_affine_matrix), \n",
    "#                   onnx_path, \n",
    "#                   export_params=True, \n",
    "#                   opset_version=20, \n",
    "#                   do_constant_folding=True, \n",
    "#                   input_names=['moving_img', 'fixed_img', 'moving_seg', 'affine_matrix'], \n",
    "#                   output_names=['moved_seg'])\n",
    "\n",
    "# print(f\"Full model has been converted to {onnx_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "95af3059-027f-4a8a-a21e-a070117fafdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved Segmentation Shape: (1, 1, 256, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "# #讀取資料並預測(C2FViT)(affine transform)\n",
    "# import onnxruntime as ort\n",
    "# import numpy as np\n",
    "# import nibabel as nib\n",
    "# from Functions import min_max_norm, pad_to_shape, reorient_image\n",
    "\n",
    "# # 讀取資料\n",
    "# moving_nii = nib.load('/NFS/PeiMao/dataset/ABIDE_NoAffine/ABIDE_0050424_tbet.nii.gz')\n",
    "# moving_seg_nii = nib.load('/NFS/PeiMao/dataset/ABIDE_aseg(tigerbx)/ABIDE_0050424_tbet_aseg.nii.gz')\n",
    "# fixed_nii = nib.load('/NFS/PeiMao/GitHub/C2FViT_Medical_Image/Data/MNI152_T1_1mm_brain_pad_RSP_RAS.nii.gz')\n",
    "# fixed_affine = fixed_nii.affine\n",
    "# fixed_header = fixed_nii.header\n",
    "\n",
    "# # 重定向moving影像的方向（R, A, S）\n",
    "# moving_nii = reorient_image(moving_nii, ('R', 'A', 'S'))\n",
    "# moving_data = moving_nii.get_fdata().astype(np.float32)\n",
    "# moving_seg_nii = reorient_image(moving_seg_nii, ('R', 'A', 'S'))\n",
    "# moving_seg_data = moving_seg_nii.get_fdata().astype(np.float32)\n",
    "# fixed_data = fixed_nii.get_fdata().astype(np.float32)\n",
    "\n",
    "# # 填充形狀至 256x256x256\n",
    "# moving_data = pad_to_shape(moving_data, (256, 256, 256))\n",
    "# moving_seg_data = pad_to_shape(moving_seg_data, (256, 256, 256))\n",
    "\n",
    "# # 固定影像的值範圍限制在一定範圍內\n",
    "# fixed_data = np.clip(fixed_data, a_min=2500, a_max=np.max(fixed_data))\n",
    "# #fixed_data = pad_to_shape(fixed_data, (256, 256, 256))\n",
    "\n",
    "# # 添加 batch 和 channel 維度\n",
    "# moving = np.expand_dims(np.expand_dims(moving_data, axis=0), axis=1)\n",
    "# moving_seg = np.expand_dims(np.expand_dims(moving_seg_data, axis=0), axis=1)\n",
    "# fixed = np.expand_dims(np.expand_dims(fixed_data, axis=0), axis=1)\n",
    "\n",
    "# # 正規化\n",
    "# moving = min_max_norm(moving)\n",
    "# fixed = min_max_norm(fixed)\n",
    "\n",
    "# # 創建 ONNX Runtime 會話\n",
    "# session = ort.InferenceSession(\"mprage_affine_transform_v001_train.onnx\")\n",
    "\n",
    "# # 獲取輸入和輸出名稱\n",
    "# input_names = [input.name for input in session.get_inputs()]\n",
    "# output_names = [output.name for output in session.get_outputs()]\n",
    "\n",
    "# # 定義仿射矩陣\n",
    "# affine_matrix = [[[0.8344236, 0.24871936, 0.01360709, 0.01237769],\n",
    "#                                      [-0.18627378, 0.9458701, -0.00467041, -0.01208516],\n",
    "#                                      [-0.014078, 0.03331293, 0.71046937, 0.00340794]]]\n",
    "\n",
    "\n",
    "# # 構建輸入字典\n",
    "# inputs = {\n",
    "#     input_names[0]: moving, \n",
    "#     input_names[1]: fixed, \n",
    "#     input_names[2]: moving_seg, \n",
    "#     input_names[3]: np.array(affine_matrix, dtype=np.float32)\n",
    "# }\n",
    "\n",
    "# # 运行模型推理\n",
    "# outputs = session.run(None, inputs)\n",
    "\n",
    "# # 輸出結果\n",
    "# moved_seg = outputs[0]\n",
    "\n",
    "# # 打印输出形状\n",
    "# print(f\"Moved Segmentation Shape: {moved_seg.shape}\")\n",
    "\n",
    "# # 使用 squeeze 移除长度为 1 的维度\n",
    "# moved_seg = np.squeeze(moved_seg)\n",
    "\n",
    "# # 創建一個 NIfTI 圖像對象\n",
    "# moved_seg_nii = nib.Nifti1Image(moved_seg, fixed_affine)\n",
    "\n",
    "# # 保存 NIfTI 圖像為 .nii.gz 文件\n",
    "# nib.save(moved_seg_nii, 'onnx_output_seg_test.nii.gz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b98c4e4b-aa49-431c-9111-101291359402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_grid_sample(im, grid, align_corners=False):\n",
    "    n, c, d, h, w = im.shape\n",
    "\n",
    "    gn, gd, gh, gw, _ = grid.shape\n",
    "\n",
    "    assert n == gn\n",
    "\n",
    "    x = grid[:, :, :, :, 0]\n",
    "    y = grid[:, :, :, :, 1]\n",
    "    z = grid[:, :, :, :, 2]\n",
    "\n",
    "    if align_corners:\n",
    "        x = ((x + 1) / 2) * (w - 1)\n",
    "        y = ((y + 1) / 2) * (h - 1)\n",
    "        z = ((z + 1) / 2) * (d - 1)\n",
    "    else:\n",
    "        x = ((x + 1) * w - 1) / 2\n",
    "        y = ((y + 1) * h - 1) / 2\n",
    "        z = ((z + 1) * d - 1) / 2\n",
    "\n",
    "    x = x.view(n, -1)\n",
    "    y = y.view(n, -1)\n",
    "    z = z.view(n, -1)\n",
    "\n",
    "    x0 = torch.floor(x).long()\n",
    "    y0 = torch.floor(y).long()\n",
    "    z0 = torch.floor(z).long()\n",
    "    x1 = x0 + 1\n",
    "    y1 = y0 + 1\n",
    "    z1 = z0 + 1\n",
    "\n",
    "    wa = ((x1 - x) * (y1 - y) * (z1 - z)).unsqueeze(1)\n",
    "    wb = ((x1 - x) * (y - y0) * (z1 - z)).unsqueeze(1)\n",
    "    wc = ((x - x0) * (y1 - y) * (z1 - z)).unsqueeze(1)\n",
    "    wd = ((x - x0) * (y - y0) * (z1 - z)).unsqueeze(1)\n",
    "    we = ((x1 - x) * (y1 - y) * (z - z0)).unsqueeze(1)\n",
    "    wf = ((x1 - x) * (y - y0) * (z - z0)).unsqueeze(1)\n",
    "    wg = ((x - x0) * (y1 - y) * (z - z0)).unsqueeze(1)\n",
    "    wh = ((x - x0) * (y - y0) * (z - z0)).unsqueeze(1)\n",
    "\n",
    "    # Apply default for grid_sample function zero padding\n",
    "    im_padded = F.pad(im, pad=[1, 1, 1, 1, 1, 1], mode='constant')\n",
    "    padded_d = d + 2\n",
    "    padded_h = h + 2\n",
    "    padded_w = w + 2\n",
    "    # save points positions after padding\n",
    "    x0, x1, y0, y1, z0, z1 = x0 + 1, x1 + 1, y0 + 1, y1 + 1, z0 + 1, z1 + 1\n",
    "\n",
    "    # Clip coordinates to padded image size\n",
    "    device = im.device\n",
    "    x0 = torch.clamp(x0, 0, padded_w - 1)\n",
    "    x1 = torch.clamp(x1, 0, padded_w - 1)\n",
    "    y0 = torch.clamp(y0, 0, padded_h - 1)\n",
    "    y1 = torch.clamp(y1, 0, padded_h - 1)\n",
    "    z0 = torch.clamp(z0, 0, padded_d - 1)\n",
    "    z1 = torch.clamp(z1, 0, padded_d - 1)\n",
    "\n",
    "    im_padded = im_padded.view(n, c, -1)\n",
    "\n",
    "    x0_y0_z0 = (x0 + y0 * padded_w + z0 * padded_w * padded_h).unsqueeze(1).expand(-1, c, -1)\n",
    "    x0_y1_z0 = (x0 + y1 * padded_w + z0 * padded_w * padded_h).unsqueeze(1).expand(-1, c, -1)\n",
    "    x1_y0_z0 = (x1 + y0 * padded_w + z0 * padded_w * padded_h).unsqueeze(1).expand(-1, c, -1)\n",
    "    x1_y1_z0 = (x1 + y1 * padded_w + z0 * padded_w * padded_h).unsqueeze(1).expand(-1, c, -1)\n",
    "    x0_y0_z1 = (x0 + y0 * padded_w + z1 * padded_w * padded_h).unsqueeze(1).expand(-1, c, -1)\n",
    "    x0_y1_z1 = (x0 + y1 * padded_w + z1 * padded_w * padded_h).unsqueeze(1).expand(-1, c, -1)\n",
    "    x1_y0_z1 = (x1 + y0 * padded_w + z1 * padded_w * padded_h).unsqueeze(1).expand(-1, c, -1)\n",
    "    x1_y1_z1 = (x1 + y1 * padded_w + z1 * padded_w * padded_h).unsqueeze(1).expand(-1, c, -1)\n",
    "\n",
    "    Ia = torch.gather(im_padded, 2, x0_y0_z0)\n",
    "    Ib = torch.gather(im_padded, 2, x0_y1_z0)\n",
    "    Ic = torch.gather(im_padded, 2, x1_y0_z0)\n",
    "    Id = torch.gather(im_padded, 2, x1_y1_z0)\n",
    "    Ie = torch.gather(im_padded, 2, x0_y0_z1)\n",
    "    If_ = torch.gather(im_padded, 2, x0_y1_z1)\n",
    "    Ig = torch.gather(im_padded, 2, x1_y0_z1)\n",
    "    Ih = torch.gather(im_padded, 2, x1_y1_z1)\n",
    "\n",
    "    return (Ia * wa + Ib * wb + Ic * wc + Id * wd + Ie * we + If_ * wf + Ig * wg + Ih * wh).reshape(n, c, gd, gh, gw)\n",
    "\n",
    "#SPM\n",
    "def custom_nearest_grid_sample(im, grid, align_corners=False):\n",
    "    n, c, d, h, w = im.shape\n",
    "\n",
    "    gn, gd, gh, gw, _ = grid.shape\n",
    "\n",
    "    assert n == gn\n",
    "\n",
    "    x = grid[:, :, :, :, 0]\n",
    "    y = grid[:, :, :, :, 1]\n",
    "    z = grid[:, :, :, :, 2]\n",
    "\n",
    "    if align_corners:\n",
    "        x = ((x + 1) / 2) * (w - 1)\n",
    "        y = ((y + 1) / 2) * (h - 1)\n",
    "        z = ((z + 1) / 2) * (d - 1)\n",
    "    else:\n",
    "        x = ((x + 1) * w - 1) / 2\n",
    "        y = ((y + 1) * h - 1) / 2\n",
    "        z = ((z + 1) * d - 1) / 2\n",
    "\n",
    "    x = torch.round(x).long()\n",
    "    y = torch.round(y).long()\n",
    "    z = torch.round(z).long()\n",
    "\n",
    "    # Clip coordinates to image size\n",
    "    x = torch.clamp(x, 0, w - 1)\n",
    "    y = torch.clamp(y, 0, h - 1)\n",
    "    z = torch.clamp(z, 0, d - 1)\n",
    "    #print(\"im.shape\", im[:, :, z, y, x].shape)\n",
    "    return im[:, :, z, y, x][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb053266-ccf1-4d69-b9de-12bd4b823e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model has been converted to mprage_affine_transform_v001_train_custom.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1367/3976391069.py:87: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert n == gn\n",
      "/usr/local/lib/python3.10/dist-packages/torch/onnx/symbolic_opset9.py:5645: UserWarning: Exporting aten::index operator of advanced indexing in opset 20 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#model轉onnx(C2FViT)(affine transform)(要輸入init_flow)(nearest)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from C2FViT_model import AffineCOMTransform, Center_of_mass_initial_pairwise\n",
    "from Functions import min_max_norm, pad_to_shape\n",
    "\n",
    "class FullModel(nn.Module):\n",
    "    def __init__(self, init_center):\n",
    "        super(FullModel, self).__init__()\n",
    "\n",
    "    def forward(self, moving_seg, init_flow, affine_matrix):\n",
    "        # Apply the grid sample for center of mass alignment\n",
    "        moving_seg = F.grid_sample(moving_seg, init_flow, mode='nearest', align_corners=True)\n",
    "        #moving_seg = custom_nearest_grid_sample(moving_seg, init_flow, align_corners=True)\n",
    "        \n",
    "        # Generate the affine grid and apply the affine transformation\n",
    "        F_X_Y = F.affine_grid(affine_matrix, moving_seg.shape, align_corners=True)\n",
    "        moving_seg = F.grid_sample(moving_seg, F_X_Y, mode='nearest', align_corners=True)\n",
    "        #moving_seg = custom_nearest_grid_sample(moving_seg, F_X_Y, align_corners=True)\n",
    "        \n",
    "        return moving_seg\n",
    "\n",
    "# 設定裝置\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 定義轉換器\n",
    "init_center = Center_of_mass_initial_pairwise()\n",
    "#init_center = CustomCenter_of_mass_initial_pairwise()\n",
    "\n",
    "# 將轉換器打包成完整模型\n",
    "full_model = FullModel(init_center).to(device)\n",
    "\n",
    "#Segmentation, and affine matrix for ONNX conversion\n",
    "dummy_moving_seg = torch.randint(0, 2, (1, 1, 256, 256, 256)).float().to(device)  # Assuming binary segmentation\n",
    "dummy_init_flow = torch.randn(1, 256, 256, 256, 3).float().to(device)\n",
    "dummy_affine_matrix = torch.tensor([[[0.8344236, 0.24871936, 0.01360709, 0.01237769],\n",
    "                                     [-0.18627378, 0.9458701, -0.00467041, -0.01208516],\n",
    "                                     [-0.014078, 0.03331293, 0.71046937, 0.00340794]]]).to(device)  # Shape (1, 3, 4)\n",
    "\n",
    "# 將完整模型轉換成 ONNX 格式\n",
    "onnx_path = \"mprage_affine_transform_v001_train.onnx\"\n",
    "torch.onnx.export(full_model, \n",
    "                  (dummy_moving_seg, dummy_init_flow, dummy_affine_matrix), \n",
    "                  onnx_path, \n",
    "                  export_params=True, \n",
    "                  opset_version=20, \n",
    "                  do_constant_folding=True, \n",
    "                  input_names=['moving_seg', 'init_flow', 'affine_matrix'], \n",
    "                  output_names=['moved_seg'])\n",
    "\n",
    "print(f\"Full model has been converted to {onnx_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e10b58e-f82a-469c-9b92-5b537d89c23c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1367/3976391069.py:6: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert n == gn\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model has been converted to mprage_affine_transform_v001_train_bili_custom.onnx\n"
     ]
    }
   ],
   "source": [
    "#model轉onnx(C2FViT)(affine transform)(要輸入init_flow)(bilinear)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from C2FViT_model import AffineCOMTransform, Center_of_mass_initial_pairwise\n",
    "from Functions import min_max_norm, pad_to_shape\n",
    "\n",
    "class FullModel(nn.Module):\n",
    "    def __init__(self, init_center):\n",
    "        super(FullModel, self).__init__()\n",
    "\n",
    "    def forward(self, moving_seg, init_flow, affine_matrix):\n",
    "        # Apply the grid sample for center of mass alignment\n",
    "        moving_seg = F.grid_sample(moving_seg, init_flow, mode='bilinear', align_corners=True)\n",
    "        #moving_seg = custom_grid_sample(moving_seg, init_flow, align_corners=True)\n",
    "        \n",
    "        # Generate the affine grid and apply the affine transformation\n",
    "        F_X_Y = F.affine_grid(affine_matrix, moving_seg.shape, align_corners=True)\n",
    "        moving_seg = F.grid_sample(moving_seg, F_X_Y, mode='bilinear', align_corners=True)\n",
    "        #moving_seg = custom_grid_sample(moving_seg, F_X_Y, align_corners=True)\n",
    "        \n",
    "        return moving_seg\n",
    "\n",
    "# 設定裝置\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 定義轉換器\n",
    "init_center = Center_of_mass_initial_pairwise()\n",
    "init_center = CustomCenter_of_mass_initial_pairwise()\n",
    "\n",
    "# 將轉換器打包成完整模型\n",
    "full_model = FullModel(init_center).to(device)\n",
    "\n",
    "#Segmentation, and affine matrix for ONNX conversion\n",
    "dummy_moving_seg = torch.randint(0, 2, (1, 1, 256, 256, 256)).float().to(device)  # Assuming binary segmentation\n",
    "dummy_init_flow = torch.randn(1, 256, 256, 256, 3).float().to(device)\n",
    "dummy_affine_matrix = torch.tensor([[[0.8344236, 0.24871936, 0.01360709, 0.01237769],\n",
    "                                     [-0.18627378, 0.9458701, -0.00467041, -0.01208516],\n",
    "                                     [-0.014078, 0.03331293, 0.71046937, 0.00340794]]]).to(device)  # Shape (1, 3, 4)\n",
    "\n",
    "# 將完整模型轉換成 ONNX 格式\n",
    "onnx_path = \"mprage_affine_transform_v001_train_bili.onnx\"\n",
    "#onnx_path = \"mprage_affine_transform_v001_train_bili_custom.onnx\"\n",
    "torch.onnx.export(full_model, \n",
    "                  (dummy_moving_seg, dummy_init_flow, dummy_affine_matrix), \n",
    "                  onnx_path, \n",
    "                  export_params=True, \n",
    "                  opset_version=20, \n",
    "                  do_constant_folding=True, \n",
    "                  input_names=['moving_seg', 'init_flow', 'affine_matrix'], \n",
    "                  output_names=['moved_seg'])\n",
    "\n",
    "print(f\"Full model has been converted to {onnx_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a865005-3021-4110-9648-ca2745747e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
