{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "910a719a-4304-4498-b156-dd013772249d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model weight ../Model/C2FViT_affine_COM_template_matching_tigerdata_RAS/C2FViT_affine_COM_template_matching_tigerdata_RAS_stagelvl3_249000.pth ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_204070/998952485.py:44: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model has been converted to C2FViT_full_model.onnx\n"
     ]
    }
   ],
   "source": [
    "#model轉onnx(C2FViT)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from C2FViT_model import C2F_ViT_stage, AffineCOMTransform, Center_of_mass_initial_pairwise\n",
    "from Functions import min_max_norm, pad_to_shape\n",
    "\n",
    "class FullModel(nn.Module):\n",
    "    def __init__(self, model, affine_transform, init_center):\n",
    "        super(FullModel, self).__init__()\n",
    "        self.model = model\n",
    "        self.affine_transform = affine_transform\n",
    "        self.init_center = init_center\n",
    "\n",
    "    def forward(self, moving_img, fixed_img):\n",
    "        # Center of mass initialization\n",
    "        moving_img, init_flow = self.init_center(moving_img, fixed_img)\n",
    "        \n",
    "        # Downsample the images\n",
    "        X_down = F.interpolate(moving_img, scale_factor=0.5, mode=\"trilinear\", align_corners=True)\n",
    "        Y_down = F.interpolate(fixed_img, scale_factor=0.5, mode=\"trilinear\", align_corners=True)\n",
    "        \n",
    "        # Run the core model\n",
    "        warpped_x_list, y_list, affine_para_list = self.model(X_down, Y_down)\n",
    "        \n",
    "        # Apply the affine transformation\n",
    "        X_Y, affine_matrix = self.affine_transform(moving_img, affine_para_list[-1])\n",
    "        \n",
    "        return X_Y, affine_matrix\n",
    "\n",
    "# 設定裝置\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 定義模型\n",
    "model = C2F_ViT_stage(img_size=128, patch_size=[3, 7, 15], stride=[2, 4, 8], num_classes=12,\n",
    "                      embed_dims=[256, 256, 256], num_heads=[2, 2, 2], mlp_ratios=[2, 2, 2], qkv_bias=False,\n",
    "                      qk_scale=None, drop_rate=0., attn_drop_rate=0., norm_layer=nn.Identity,\n",
    "                      depths=[4, 4, 4], sr_ratios=[1, 1, 1], num_stages=3, linear=False).to(device)\n",
    "\n",
    "# 加載預訓練模型權重\n",
    "model_path = '../Model/C2FViT_affine_COM_template_matching_tigerdata_RAS/C2FViT_affine_COM_template_matching_tigerdata_RAS_stagelvl3_249000.pth'\n",
    "print(f\"Loading model weight {model_path} ...\")\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "# 定義轉換器\n",
    "affine_transform = AffineCOMTransform().to(device)\n",
    "init_center = Center_of_mass_initial_pairwise()\n",
    "\n",
    "# 將核心模型和轉換器打包成完整模型\n",
    "full_model = FullModel(model, affine_transform, init_center).to(device)\n",
    "\n",
    "# 加載固定影像\n",
    "fixed_path = '../Data/MNI152_T1_1mm_brain_pad_RSP.nii.gz'\n",
    "fixed_img_nii = nib.load(fixed_path)\n",
    "fixed_img = fixed_img_nii.get_fdata()\n",
    "\n",
    "# 確保影像尺寸是 256x256x256\n",
    "target_shape = (256, 256, 256)\n",
    "if fixed_img.shape != target_shape:\n",
    "    fixed_img = pad_to_shape(fixed_img, target_shape)\n",
    "\n",
    "fixed_img = min_max_norm(fixed_img)\n",
    "fixed_img = torch.from_numpy(fixed_img).float().to(device).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "# Dummy moving image for ONNX conversion\n",
    "dummy_moving_img = torch.randn(1, 1, 256, 256, 256).to(device)\n",
    "\n",
    "# 將完整模型轉換成 ONNX 格式\n",
    "onnx_path = \"C2FViT_full_model.onnx\"\n",
    "torch.onnx.export(full_model, \n",
    "                  (dummy_moving_img, fixed_img), \n",
    "                  onnx_path, \n",
    "                  export_params=True, \n",
    "                  opset_version=20, \n",
    "                  do_constant_folding=True, \n",
    "                  input_names=['moving_img', 'fixed_img'], \n",
    "                  output_names=['moved', 'affine_matrix'])\n",
    "\n",
    "print(f\"Full model has been converted to {onnx_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d1e1c50-3c75-4b2c-ad7b-058055187e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 256, 256, 256)\n",
      "(1, 3, 4)\n",
      "[[ 0.77673465  0.3878609  -0.04437534  0.01298795]\n",
      " [-0.36949837  0.8408904   0.01499392 -0.02714885]\n",
      " [ 0.05178259 -0.01343449  0.8940104   0.00496839]]\n"
     ]
    }
   ],
   "source": [
    "#讀取資料並預測\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from Functions import min_max_norm, pad_to_shape, reorient_image\n",
    "\n",
    "    \n",
    "moving_nii = nib.load('/NFS/PeiMao/GitHub/C2FViT_Medical_Image/Data/ABIDE_0050003_tbet.nii.gz')\n",
    "fixed_nii = nib.load('/NFS/PeiMao/GitHub/C2FViT_Medical_Image/Data/MNI152_T1_1mm_brain_pad_RSP_RAS.nii.gz')\n",
    "fixed_affine = fixed_nii.affine\n",
    "fixed_header = fixed_nii.header\n",
    "moving_nii = reorient_image(moving_nii, ('R', 'A', 'S'))\n",
    "moving_data = moving_nii.get_fdata().astype(np.float32)\n",
    "fixed_data = fixed_nii.get_fdata().astype(np.float32)\n",
    "moving_data = pad_to_shape(moving_data, (256, 256, 256))\n",
    "\n",
    "fixed_data = np.clip(fixed_data, a_min=2500, a_max=np.max(fixed_data))\n",
    "\n",
    "# 在第0轴和第1軸位置添加新维度（增加一个 batch size 维度）\n",
    "moving = np.expand_dims(moving_data, axis=0)\n",
    "moving = np.expand_dims(moving, axis=1)\n",
    "fixed = np.expand_dims(fixed_data, axis=0)\n",
    "fixed = np.expand_dims(fixed, axis=1)\n",
    "\n",
    "moving = min_max_norm(moving)\n",
    "fixed = min_max_norm(fixed)\n",
    "\n",
    "# 创建 ONNX Runtime 会话\n",
    "session = ort.InferenceSession(\"C2FViT_full_model.onnx\")\n",
    "\n",
    "# 获取输入的名称\n",
    "input_names = [input.name for input in session.get_inputs()]\n",
    "output_names = [output.name for output in session.get_outputs()]\n",
    "\n",
    "# 创建输入字典\n",
    "inputs = {input_names[0]: moving, input_names[1]: fixed}\n",
    "\n",
    "# 运行模型推理\n",
    "outputs = session.run(None, inputs)\n",
    "\n",
    "# 打印输出结果\n",
    "print(outputs[0].shape)\n",
    "print(outputs[1].shape)\n",
    "\n",
    "# 使用 squeeze 移除长度为 1 的维度\n",
    "moved = np.squeeze(outputs[0])\n",
    "affine_matrix = np.squeeze(outputs[1])\n",
    "print(affine_matrix)\n",
    "\n",
    "# 创建一个 NIfTI 图像对象\n",
    "moved_nii = nib.Nifti1Image(moved, fixed_affine)\n",
    "\n",
    "# 保存 NIfTI 图像为 .nii.gz 文件\n",
    "nib.save(moved_nii, 'onnx_output_image.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0029dc34-3ad7-4cfa-94c8-ce8bb706bf29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
